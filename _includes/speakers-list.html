<!-- Begin Speakers List-->
<section id="speakers" class="speakers">
    <div class="content-wrapper">
        <div class="col-lg-10 col-lg-offset-1  text-left">

            <h5><b>Yuanchun Shi</b></h5>
            <h6>Tsinghua University, China</h6>

            <table>
            <tr>
                <td>
                    <img src="{{ site.baseurl }}/img/people/YuanchunShi.jpg" height="300" width="449">
                </td>
                <td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
                <td>
                    <p><b>Bio:</b> <a href="http://media.cs.tsinghua.edu.cn/~pervasive/shiyc/" target="_blank">Prof. Yuanchun Shi</a> is a Changjiang distinguished professor of the Department of Computer Science, the director of HCI & Media Integration Institute. Her research interests include pervasive computing, human computer interaction, distributed multimedia processing and e-learning. Prof. Shi has publications in IEEE Pervasive Computing, IJHCS, TPDS, TKDE, ACM CHI, MM, UIST, etc.  She has won 2 National Science and Technology Advancement Awards and many best paper awards. Prof. Shi received all her PhD, MS and BS in Computer Science from Tsinghua University.</p>

                    <p><b>Title: Interpreting user input intention in natural human computer interaction</b></p>

                    <p><b>Abstract:</b>  Human Computer Interaction (HCI) is about information exchange between human and computers. Interaction between users and computers occurs at the User Interface (UI). Now, computers become pervasive, they are embedded in everyday things and UIs are the main value-added competitive advantages. UIs should be more natural for users. NUI (natural user interface) expands forms beyond formal input devices like the mouse and keyboard to more and more natural forms of interaction such as touch, speech, gestures, handwriting, and vision. Unlike speech, handwriting and vision, which have been researched for decades and put into practical use recently, touch and gestures are interaction tasks related, and yet lack of study. This talk will introduce methods of modeling user input action based on data with the random noise for fast touch input and natural gestures.</p>

                </td>
            </tr>
            </table> 

            <br/>
            <br/>
            <h5><b>Ana Paiva</b></h5>
            <h6>University of Lisbon, Portugal</h6>

            <table>
            <tr>
                <td>
                    <p><b>Bio:</b> <a href="https://gaips.inesc-id.pt/component/gaips/people/displayPerson/8/14/" target="_blank">Prof. Ana Paiva</a> is a  Full Professor in the Department of Computer Engineering, IST (“Instituto Superior Técnico”) from the University of Lisbon and coordinator of GAIPS – “Intelligent and Social Agents Group” at INESC-ID.  She investigates the creation of AI and complex systems using an agent-based approach, with a particular focus on social agents. Her primary research interests are in the fields of Autonomous Agents and Multi-Agent Systems, Affective Computing, Virtual Agents and Human-Robot Interaction. Her most recent work focuses on the affective and social elements present in the interactions between humans and robots more specifically on how to engineer natural and positive relationships with social robots.
                    </p>
                    <p>For the past twenty years she has served in the organization of numerous international conference and workshops (including, AAMAS, HRI, ACII), and (co-)authored over 200 publications in refereed journals, conferences and books</p>

                    <p><b>Title: Robots that listen to people's hearts: the role of emotions in the communication between humans and social robots</b></p>

                    <p><b>Abstract:</b>As robots begin to integrate our world and invade our streets and homes, they must act as autonomous and intelligent beings. However, so far, they are deprived of our responsive and emotional capacities, lacking awareness of the social world we live in. In the future, robots should be able to take into account these distinctive dimensions of human social interactions to be able to act appropriately within such social contexts. To do this, they must adapt and embody the essence of social and emotional intelligence. This not only includes the ability to recognise human emotions and social interactions but also understand them, deliberate them and act accordingly.</p>

                    <p>Lately, significant research has been carried out in an attempt to find ways to build social and emotional robots that are able to perceive the user's emotions, adapt to them, and react appropriately.</p>

                    <p>This talk will therefore provide an overview of the area of emotions in social interactions established between humans and social robots. In this analysis, I will use scenarios from educational and entertainment robotics, outline the process of building emotional social robots and finally proceed to interpret the effect that such capabilities have on user's interactions, learning, motivation, relationship and trust.</p>

                    <p>I believe that by studying and engineering emotional and social interactions "for" and "with" robots, we have the opportunity to build a new generation of natural, engaging, effective and, most importantly, "humane" AI.</p>
                    
                </td>
                <td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
                <td>
                    <img src="{{ site.baseurl }}/img/people/Ana.jpg" height="300" width="449">

                </td>
            </tr>
            </table> 

            <br/>
            <br/>

            <h5><b>Barry Smyth</b></h5>
            <h6>University College Dublin, Ireland</h6>

            <table>
            <tr>
                <td>
                    <img src="{{ site.baseurl }}/img/people/Barry.jpg" height="451" width="300">
                </td>
                <td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
                <td>
                    <p><b>Bio:</b> <a href="http://barrysmyth.me" target="_blank">Prof. Barry Smyth</a> (BSc, PhD, Hon. DTech(RGU), MRIA) is a scientist and entrepreneur. He holds the DIGITAL Chair of Computer Science at University College Dublin. He is a member of the Royal Irish Academy and a Founding Director of the Insight Centre for Data Analytics. Barry's research interests cover a broad range of topics, including artificial intelligence, case-based reasoning, and recommender systems. Barry is also an entrepreneur. He co-founded ChangingWorlds and HeyStaks, based on research from his lab, and he currently serves as a board member and/or advisor for a number of AI-related start-ups.</p>

                    <p><b>Title: Running Recommendations</b></p>

                    <p><b>Abstract:</b> The history of personalisation and recommender systems is, in large part, a web-tale: a story of sites and services that learn about users, in order to provide more tailored experiences. The rapid rise of mobile computing, combined with wearable sensors, and an increasingly connected IoT world, has begun to shift the potential for personalisation, from the virtual world of the web, to the physical world in which we live, work, and play. This talk will consider exciting new application opportunities for user modelling, personalisation, and recommendation in the area of personal health and fitness, with a particular emphasis on how these technologies can help people to exercise more effectively, and by drawing from recent results for marathon runners.</p>

                </td>
            </tr>
            </table> 
   
        </div>
    </div>
</section>
<!-- End Speakers List -->